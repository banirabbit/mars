# src/multiagent_recommender.py

import ast
import json
import re
from typing import Dict, List, Tuple, TypedDict
from langgraph.graph import StateGraph, END
import os
from langchain_core.runnables import RunnableLambda
from openai import OpenAI

# Global variables for OpenAI client and model - will be initialized by setup_llm_client()
CLIENT = None
MODEL = None
MAX_RETRY_COUNT = 5


def setup_llm_client(base_url, api_key, model_name, max_retry_count=5):
    """Setup OpenAI client and model from configuration."""
    global CLIENT, MODEL, MAX_RETRY_COUNT
    CLIENT = OpenAI(
        base_url=base_url,
        api_key=api_key,
    )
    MODEL = model_name
    MAX_RETRY_COUNT = max_retry_count


def select_top_apis(state):
    apis = state["candidate_apis"]
    mashups = state["mashup"]
    core_prompt = state["prompt"]
    feedback = state.get("feedback_reason", "")

    prompt_payload = {"mashup": mashups, "candidate_apis": apis}

    if feedback:
        prompt_payload["feedback"] = feedback

    user_prompt = json.dumps(prompt_payload, ensure_ascii=False)

    messages = [
        {"role": "system", "content": core_prompt},
        {"role": "user", "content": user_prompt},
    ]

    if CLIENT is None or MODEL is None:
        raise RuntimeError("LLM client not initialized. Call setup_llm_client() first.")

    completion = CLIENT.chat.completions.create(
        model=MODEL, messages=messages, stream=False
    )

    response = completion.choices[0].message.content
    print(response)
    return {
        **state,
        "related_apis": process_json(response),
        "feedback_reason": "",  # æ¸…é™¤æ—§çš„åé¦ˆ
    }


# def process_json(text):
#     pattern = r"```json(.*?)```"
#     matches = re.findall(pattern, text, re.DOTALL)
#     for match in matches:
#         try:
#             json_data = json.loads(match.strip())  # è½¬æ¢ä¸º JSON å¯¹è±¡
#             # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ "related_apis" é”®
#             if "related_apis" not in json_data:
#                 print("è§£æçš„ JSON ç¼ºå°‘ 'related_apis' é”®")
#                 return []
#             return json_data["related_apis"]
#         except json.JSONDecodeError as e:
#             print("è§£æå¤±è´¥:", e)
#             return []
#         except Exception as e:
#             print(f"æ²¡æœ‰æ‰¾åˆ°JSONçš„å¼€å§‹æˆ–ç»“æŸç¬¦: {e}")
#             return []
#     print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½• JSON ä»£ç å—")
#     return []

def process_json(text: str):
    """
    åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾ JSONï¼ˆæ”¯æŒ ```json ... ``` ä»£ç å—å’Œå†…è” { ... } ç»“æ„ï¼‰ï¼Œ
    è§£æå¹¶è¿”å›å…¶ä¸­æ‰€æœ‰å‡ºç°çš„ related_apisï¼ˆå»é‡åæŒ‰å‡ºç°é¡ºåºè¿”å›ï¼‰ã€‚
    å‘ä¸‹å…¼å®¹ï¼šå¦‚æœåªå­˜åœ¨ä»£ç å—é‡Œçš„ JSONï¼Œè¡Œä¸ºç­‰åŒäºåŸå…ˆç‰ˆæœ¬ã€‚
    """
    candidates = []

    # 1) å…ˆæ‰¾ ```json ... ``` ä»£ç å—ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    fenced_pattern = r"```json\s*(.*?)\s*```"
    fenced_matches = re.findall(fenced_pattern, text, flags=re.DOTALL | re.IGNORECASE)
    candidates.extend([m.strip() for m in fenced_matches])

    # 2) å†æ‰«æå†…è” { ... }ï¼Œç”¨æ‹¬å·åŒ¹é…æ‰¾å‡ºå¯èƒ½çš„ JSON å­ä¸²
    #    åªæ”¶å½•åŒ…å« related_apis å…³é”®å­—çš„å­ä¸²ï¼Œå‡å°‘è¯¯ä¼¤
    for i, ch in enumerate(text):
        if ch == '{':
            stack = 1
            j = i + 1
            while j < len(text) and stack > 0:
                if text[j] == '{':
                    stack += 1
                elif text[j] == '}':
                    stack -= 1
                j += 1
            if stack == 0:
                snippet = text[i:j]
                if "related_apis" in snippet:
                    candidates.append(snippet)

    # å»é‡ï¼ˆæŒ‰æ–‡æœ¬ç‰‡æ®µï¼‰
    seen = set()
    candidates = [c for c in candidates if not (c in seen or seen.add(c))]

    results = []
    def try_parse(candidate: str):
        """å°½é‡æŠŠ candidate è§£ææˆ dictï¼›åšä¸€äº›å¸¸è§å®¹é”™ã€‚"""
        # ç›´æ¥å°è¯• json.loads
        try:
            return json.loads(candidate)
        except json.JSONDecodeError:
            pass

        # å°è¯•ä¿®æ­£ï¼šä¸­è‹±æ–‡å¼•å·ã€å°¾é€—å·
        fixed = (candidate
                 .replace('â€œ', '"').replace('â€', '"')
                 .replace('â€²', "'").replace('â€™', "'"))
        fixed = re.sub(r",\s*([}\]])", r"\1", fixed)
        try:
            return json.loads(fixed)
        except Exception:
            pass

        # æœ€åå°è¯• ast.literal_evalï¼ˆå¯¹ â€œç±» JSONâ€ ä¹Ÿæ›´å®½å®¹ï¼‰
        try:
            return ast.literal_eval(candidate)
        except Exception as e:
            print("è§£æå¤±è´¥:", e)
            return None

    # è§£æå€™é€‰å¹¶æ”¶é›† related_apis
    for cand in candidates:
        data = try_parse(cand)
        if not isinstance(data, dict):
            continue
        if "related_apis" not in data:
            # ä¸åŸå®ç°ä¿æŒç›¸è¿‘çš„æç¤º
            print("è§£æçš„ JSON ç¼ºå°‘ 'related_apis' é”®")
            continue
        apis = data.get("related_apis")
        if isinstance(apis, list):
            results.extend(apis)

    if not results:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½• JSON ä»£ç å—æˆ–åŒ…å« related_apis çš„å¯¹è±¡")
        return []

    # å»é‡å¹¶ä¿æŒé¡ºåº
    out, s = [], set()
    for a in results:
        if a not in s:
            out.append(a)
            s.add(a)
    return out


def check_recommendation_validity(state) -> Tuple[Dict, str]:
    mashup = state["mashup"]
    candidates = state["candidate_apis"]
    recommended = state["related_apis"]
    prompt = state["prompt"]
    retry_count = state.get("retry_count", 0)

    user_input = json.dumps(
        {
            "mashup": mashup,
            "candidate_apis": candidates,
            "recommended_apis": recommended,
        },
        ensure_ascii=False,
    )

    messages = [
        {
            "role": "system",
            "content": (
                prompt
                + "\nYou are an expert assistant evaluating the quality of API recommendations."
                "\nPlease judge whether the recommended APIs satisfy the mashup's core requirements."
                "\nIf the recommendations are valid, respond in the following JSON format:\n"
                '```json\n{"valid": true}\n```\n'
                "If the recommendations are invalid, respond as:\n"
                '```json\n{"valid": false, "reason": "<brief explanation why the APIs do not meet the requirements>"}\n```\n'
            ),
        },
        {"role": "user", "content": user_input},
    ]

    if CLIENT is None or MODEL is None:
        raise RuntimeError("LLM client not initialized. Call setup_llm_client() first.")

    completion = CLIENT.chat.completions.create(
        model=MODEL, messages=messages, stream=False
    )

    response = completion.choices[0].message.content.strip()
    print("[ğŸ” validity check response]:", response)

    # æå–æ¨¡å‹è¾“å‡ºçš„ JSON
    pattern = r"```json(.*?)```"
    match = re.search(pattern, response, re.DOTALL)
    if match:
        try:
            result = json.loads(match.group(1).strip())
            if result.get("valid") is True:
                return {**state, "is_retry": False}
            else:
                reason = result.get(
                    "reason",
                    "The recommended APIs do not match the mashup requirements.",
                )
                return {
                    **state,
                    "is_retry": True,
                    "retry_count": retry_count + 1,
                    "feedback_reason": reason,
                }
        except Exception as e:
            print("[âŒ JSONè§£æå¤±è´¥]:", e)
            return {
                **state,
                "is_retry": True,
                "retry_count": retry_count + 1,
                "feedback_reason": "Fail to parse JSON.",
            }

    # fallbackï¼Œå¦‚æœæ²¡æœ‰æ­£ç¡®æ ¼å¼ï¼Œå°±é»˜è®¤ç»“æŸ
    return {**state, "is_retry": False}

def revise_prompt(state):
    mashup = state["mashup"]
    original_prompt = state["prompt"]
    feedback = state.get("feedback_reason", "")
    previous_apis = state.get("related_apis", [])

    messages = [
        {"role": "system", "content": "You are an expert in writing prompts for API selection."},
        {"role": "user", "content": (
            f"The original prompt was:\n{original_prompt}\n\n"
            f"The model failed to generate useful APIs. Feedback:\n{feedback}\n\n"
            f"The mashup description and categories are {mashup}"
            f"The previous APIs were:\n{previous_apis}\n\n"
            f"Please revise the original prompt to be more helpful, specific, or constrained."
            f"Please preserve the structure of the original prompt while improving the clarity or completeness to better match the mashup's requirements."
            f"Output only the revised prompt."
        )}
    ]

    if CLIENT is None or MODEL is None:
        raise RuntimeError("LLM client not initialized. Call setup_llm_client() first.")

    response = CLIENT.chat.completions.create(
        model=MODEL,
        messages=messages
    )

    new_prompt = response.choices[0].message.content.strip()
    print("[ğŸ”§ prompt updated]:", new_prompt)

    return {
        **state,
        "prompt": new_prompt
    }
# step 3: åˆ†æ”¯é€»è¾‘å‡½æ•°ï¼ˆå†³å®šè·³è½¬æ–¹å‘ï¼‰
def route_based_on_validity(state) -> str:
    global MAX_RETRY_COUNT
    if state.get("retry_count", 0) >= MAX_RETRY_COUNT:
        print(f"[âš ï¸ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° {MAX_RETRY_COUNT}ï¼Œç»ˆæ­¢æµç¨‹]")
        return "end"
    return "retry" if state.get("is_retry") else "end"


# å®šä¹‰çŠ¶æ€ç»“æ„
class WorkflowState(TypedDict):
    mashup: str
    candidate_apis: List[Dict]
    related_apis: List[str]
    prompt: str
    is_retry: bool
    retry_count: int


def run_multiagent_flow(mashup_description, candidate_apis, prompt):
    workflow = StateGraph(state_schema=WorkflowState)

    workflow.add_node("select_top_apis", select_top_apis)
    # workflow.add_node("revise_prompt", revise_prompt)
    workflow.add_node(
        "check_recommendation_validity", RunnableLambda(check_recommendation_validity)
    )

    workflow.set_entry_point("select_top_apis")

    # å¤šè·¯åˆ†æ”¯æ ¹æ®æ ¡éªŒç»“æœè·³è½¬
    workflow.add_edge("select_top_apis", "check_recommendation_validity")
    workflow.add_conditional_edges("check_recommendation_validity", route_based_on_validity, {
    "retry": "select_top_apis",
    "end": END,
    })
    # workflow.add_conditional_edges("check_recommendation_validity", route_based_on_validity, {
    # "retry": "revise_prompt",
    # "end": END,
    # })
    # workflow.add_edge("revise_prompt", "select_top_apis")

    graph = workflow.compile()

    input_data = {
        "mashup": mashup_description,
        "candidate_apis": candidate_apis,
        "prompt": prompt,
        "related_apis": [],
        "is_retry": False,
        "retry_count": 0,
    }

    result = graph.invoke(input_data)
    return result
